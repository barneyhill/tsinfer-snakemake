import os.path
import dask
from pathlib import Path
from dask.distributed import Client, performance_report

configfile: "config.yaml"
shell.prefix(config["prefix"])

for k,v in config["dask"].items():
    dask.config.set({k:v})

data_dir=Path(config['data_dir'])

# These are quick steps that we don't want to submit jobs for
localrules: all, summary_table

rule all:
    input:
        data_dir/"vcf_summary_table.csv",
        expand(data_dir/"ancestors"/"{vcf_name}-subset-{subset}"/"ancestors.trees",
            vcf_name=config["vcfs"].keys(),
            subset=[config['sample_ids_file'].replace("/", "--")]
        )


rule vcf_to_zarrs:
    input:
        vcf=lambda wildcards: config["vcfs"][wildcards.vcf_name]['vcf'],
        tbi=lambda wildcards: config["vcfs"][wildcards.vcf_name]['vcf']+".tbi"
    output:
        data_dir/"zarr_vcfs"/"{vcf_name}"/"data.zarr"/".vcf_done",
        data_dir/"zarr_vcfs"/"{vcf_name}"/"performance_report.html"
    resources:
        dask_cluster=10,
        mem_mb=16000,
        time_min=239,
        runtime=239
    params:
        target_part_size="5M",
        read_chunk_length=10_000,
        temp_chunk_length=20_000,
        chunk_length=40_000,
        chunk_width=1_500,
        retain_temp_files=True
    run:
        import sgkit.io.vcf
        client = Client(config["scheduler_address"])
        with performance_report(
            filename=output[1]
        ):
            sgkit.io.vcf.vcf_reader.vcf_to_zarr(
                input[0],
                output[0].replace(".vcf_done", ""),
                target_part_size=params.target_part_size,
                read_chunk_length=params.read_chunk_length,
                temp_chunk_length=params.temp_chunk_length,
                chunk_length=params.chunk_length,
                chunk_width=params.chunk_width,
                tempdir=config["temp_dir"],
                retain_temp_files=params.retain_temp_files
            )
        Path(str(output[0])).touch()

rule load_ancestral_fasta:
    input:
        lambda wildcards: config["vcfs"][wildcards.vcf_name.split("--")[0]]['ancestral_fasta'],
        data_dir/"zarr_vcfs"/"{vcf_name}"/"data.zarr"/".vcf_done"
    output:
        directory(data_dir/"zarr_vcfs"/"{vcf_name}"/"data.zarr"/"variant_ancestral_allele")
    threads: 1
    resources:
        mem_mb=16000,
        time_min=30,
        runtime=30
    run:
        import pyfaidx
        import numpy
        import sgkit
        import xarray
        fasta = pyfaidx.Fasta(input[0])
        seq_name = list(fasta.keys())[0]
        print("Ancestral sequence:", seq_name)
        ancestral_sequence = numpy.asarray(fasta[seq_name], dtype="U1")
        # From the human ancestral states README:
        # The convention for the sequence is:
        #    ACTG : high-confidence call, ancestral state supported by other 2 sequences
        #    actg : low-confidence call, ancestral state supported by one sequence only
        #    N    : failure, the ancestral state is not supported by any other sequence
        #    -    : the extant species contains an insertion at this position
        #    .    : no coverage in the alignment
        ds_dir = input[1].replace(".vcf_done", "")
        ds = sgkit.load_dataset(ds_dir)
        ancestral_states = numpy.char.upper(ancestral_sequence[ds['variant_position'].values-1])
        ancestral_states = xarray.DataArray(data=ancestral_states, dims=["variants"], name="variant_ancestral_allele")
        ds.update({"variant_ancestral_allele": ancestral_states})
        sgkit.save_dataset(ds.drop_vars(set(ds.data_vars) - {"variant_ancestral_allele"}), ds_dir, mode="a")

rule mask_sites:
    input:
        data_dir/"zarr_vcfs"/"{vcf_name}"/"data.zarr"/".vcf_done",
        data_dir/"zarr_vcfs"/"{vcf_name}"/"data.zarr"/"variant_ancestral_allele"
    output:
        directory(data_dir/"zarr_vcfs"/"{vcf_name}"/"data.zarr"/"variant_mask"),
        directory(data_dir/"zarr_vcfs"/"{vcf_name}"/"data.zarr"/"variant_duplicate_position_mask"),
        directory(data_dir/"zarr_vcfs"/"{vcf_name}"/"data.zarr"/"variant_bad_ancestral_mask"),
        directory(data_dir/"zarr_vcfs"/"{vcf_name}"/"data.zarr"/"variant_no_ancestral_allele_mask")
    resources:
        mem_mb=16000,
        time_min=30,
        runtime=30
    run:
        #No need for a dask cluster here
        from xarray import full_like
        import sgkit
        import numpy
        import sgkit
        ds_dir = input[0].replace(".vcf_done", "")
        ds = sgkit.load_dataset(ds_dir)
        chunks = ds.variant_position.chunks

        # Mask bad ancestral sites
        wanted_variants = (ds['variant_ancestral_allele'] != '-') & (ds['variant_ancestral_allele'] != '.') & (ds['variant_ancestral_allele'] != 'N')
        wanted_variants = wanted_variants.rename("variant_bad_ancestral_mask").chunk(chunks).compute()
        assert set(numpy.unique(ds['variant_ancestral_allele'][wanted_variants])) == {'A', 'C', 'G', 'T'}
        ds.update({"variant_bad_ancestral_mask": wanted_variants})
        sgkit.save_dataset(ds.drop_vars(set(ds.data_vars) - {"variant_bad_ancestral_mask"}), ds_dir, mode="a")

        # Mask sites where the ancestral state is not an allele
        wanted_variants = ((ds['variant_ancestral_allele'] == ds['variant_allele'][:,0]) |
                           (ds['variant_ancestral_allele'] == ds['variant_allele'][:,1]))
        wanted_variants = wanted_variants.rename("variant_no_ancestral_allele_mask").chunk(chunks).compute()
        ds.update({"variant_no_ancestral_allele_mask": wanted_variants})
        sgkit.save_dataset(ds.drop_vars(set(ds.data_vars) - {"variant_no_ancestral_allele_mask"}), ds_dir, mode="a")

        # Mask duplicate sites with duplicate position
        pos = ds['variant_position']
        pos_shift_left = full_like(pos,-1)
        pos_shift_left[0:-1] = pos[1:]
        pos_shift_right = full_like(pos,-1)
        pos_shift_right[1:] = pos[:-1]
        wanted_variants = (pos != pos_shift_left) & (pos != pos_shift_right)
        wanted_variants = wanted_variants.rename("variant_duplicate_position_mask").chunk(chunks).compute()
        ds.update({"variant_duplicate_position_mask": wanted_variants})
        sgkit.save_dataset(ds.drop_vars(set(ds.data_vars) - {"variant_duplicate_position_mask"}), ds_dir, mode="a")

        ## Create the combined mask - tsinfer now copes with missing and bad ancestral sites so don't use those masks
        wanted_variants = ds['variant_duplicate_position_mask']
        wanted_variants = wanted_variants.rename("variant_mask").chunk(chunks).compute()
        ds.update({"variant_mask": wanted_variants})
        sgkit.save_dataset(ds.drop_vars(set(ds.data_vars) - {"variant_mask"}), ds_dir, mode="a")

rule zarr_stats:
    input:
        data_dir/"zarr_vcfs"/"{vcf_name}"/"data.zarr"/".vcf_done",
        data_dir/"zarr_vcfs"/"{vcf_name}"/"data.zarr"/"variant_mask",
        data_dir/"zarr_vcfs"/"{vcf_name}"/"data.zarr"/"variant_duplicate_position_mask",
        data_dir/"zarr_vcfs"/"{vcf_name}"/"data.zarr"/"variant_bad_ancestral_mask",
        data_dir/"zarr_vcfs"/"{vcf_name}"/"data.zarr"/"variant_no_ancestral_allele_mask"
    output:
        data_dir/"zarr_stats"/"{vcf_name}"/"stats.json"
    resources:
        dask_cluster=5,
        mem_mb=16000,
        time_min=30,
        runtime=30
    run:
        import sgkit
        import json
        client = Client(config["scheduler_address"])
        ds = sgkit.load_dataset(input[0].replace(".vcf_done", ""))
        out = {}
        out['dataset_summary'] = str(ds)
        out['name'] = ', '.join(ds.attrs['contigs'])
        out['n_samples'] = ds.dims['samples']
        out['n_variants'] = ds.dims['variants']
        out['n_ploidy'] = ds.dims['ploidy']
        #Flatten the ploidy dimension as tsinfer sees each phased haplotype as a sample
        gt = (ds.call_genotype.stack(haplotype=("samples","ploidy")))
        out['allele_counts'] = (gt > 0).sum(dim=['haplotype']).to_numpy().tolist()
        out['missing_count'] = int((gt == -1).sum())
        out['num_sites_triallelic'] = int(((gt > 1).sum(dim=["haplotype"]) > 0).sum())
        out['sites_bad_ancestral'] = int((~ds.variant_bad_ancestral_mask).sum())
        out['sites_no_ancestral'] = int((~ds.variant_no_ancestral_allele_mask).sum())
        out['sites_duplicate_pos'] = int((~ds.variant_duplicate_position_mask).sum())
        out['sites_masked'] = int((~ds.variant_mask).sum())
        total_size = 0
        for dirpath, dirnames, filenames in os.walk(input[0].replace(".vcf_done", "")):
            for f in filenames:
                fp = os.path.join(dirpath, f)
                total_size += os.path.getsize(fp)
        out['size'] = total_size
        with open(output[0], "w") as f:
            f.write(json.dumps(out))

checkpoint summary_table:
    input:
        expand(data_dir/"zarr_stats"/"{vcf_name}"/"stats.json", vcf_name=config["vcfs"].keys())
    output:
        data_dir/"vcf_summary_table.csv"
    run:
        import csv
        import json
        def number_to_SI(number):
            """Convert a number to a string with SI units, unless it is a string already."""
            units = ["", "K", "M", "G", "T", "P", "E", "Z", "Y"]
            unit = 0
            if isinstance(number, str):
                return number
            while number > 1000:
                number /= 1000
                unit += 1
            return f"{number:.2f}{units[unit]}"

        header = ["vcf_name", "vcf_size", "zarr_size", "n_variants", "n_samples", "ac==0", "ac==1",
        "ac==2", "missing_genotypes", "num_sites_triallelic", "sites_bad_ancestral", "sites_no_ancestral",
        "sites_duplicate_pos", "sites_masked", "inference_nbytes", "inference_bitpack_nbytes"]
        with open(output[0], "w", newline='') as f:
            writer = csv.DictWriter(f, fieldnames=header)
            writer.writeheader()

            for vcf_stats in input:
                with open(vcf_stats, "r") as json_stats_f:
                    stats = json.load(json_stats_f)
                    ac0 = sum([ac == 0 for ac in stats['allele_counts']])
                    ac1 = sum([ac == 1 for ac in stats['allele_counts']])
                    ac2 = sum([ac == 2 for ac in stats['allele_counts']])
                    n_sites = stats['n_variants']
                    n_samples = stats['n_samples']
                    n_ploidy = stats['n_ploidy']
                    n_masked = stats['sites_masked']
                    row_dict = {
                        "vcf_name": stats['name'],
                        "vcf_size": os.path.getsize(config["vcfs"][stats['name']]['vcf']),
                        "zarr_size": stats['size'],
                        "n_variants": n_sites,
                        "n_samples": n_samples,
                        "ac==0": ac0,
                        "ac==1": ac1,
                        "ac==2": ac2,
                        "missing_genotypes": stats['missing_count'],
                        "num_sites_triallelic": stats['num_sites_triallelic'],
                        "sites_bad_ancestral": stats['sites_bad_ancestral'],
                        "sites_no_ancestral": stats['sites_no_ancestral'],
                        "sites_duplicate_pos": stats['sites_duplicate_pos'],
                        "sites_masked": n_masked,
                        "inference_nbytes": ((n_sites-n_masked) - ac1) * n_samples * n_ploidy,
                        "inference_bitpack_nbytes": (((n_sites-n_masked) - ac1) * n_samples * n_ploidy) / 8
                    }
                    row_dict = {k: number_to_SI(v) for k, v in row_dict.items()}
                    writer.writerow(row_dict)

rule subset_zarr_vcf:
    input:
        data_dir/"zarr_vcfs"/"{vcf_name}"/"data.zarr"/".vcf_done",
        data_dir/"zarr_vcfs"/"{vcf_name}"/"data.zarr"/"variant_ancestral_allele",
        data_dir/"zarr_vcfs"/"{vcf_name}"/"data.zarr"/"variant_mask",
        data_dir/"zarr_vcfs"/"{vcf_name}"/"data.zarr"/"variant_duplicate_position_mask",
        data_dir/"zarr_vcfs"/"{vcf_name}"/"data.zarr"/"variant_bad_ancestral_mask",
        data_dir/"zarr_vcfs"/"{vcf_name}"/"data.zarr"/"variant_no_ancestral_allele_mask"

    output:
        directory(data_dir/"zarr_vcfs_subsets"/"{vcf_name}-subset-{sample_ids_file}"/"data.zarr")
    resources:
        dask_cluster=5,
        mem_mb=16000,
        time_min=239,
        runtime=239
    run:
        import numpy
        import sgkit
        client = Client(config["scheduler_address"])
        ds = sgkit.load_dataset(input[0].replace(".vcf_done", ""))
        with open(wildcards.sample_ids_file.replace("--", "/"),'r') as f:
            sample_ids = numpy.genfromtxt(f,dtype=str)
        mask = numpy.isin(ds.sample_id.values, sample_ids)
        ds = ds.sel(samples=mask)
        ds = ds.unify_chunks()
        sgkit.save_dataset(ds,output[0].replace(".vcf_done", ""), auto_rechunk=True)
        Path(str(output[0])).touch()

def get_ancestor_gen_memory(wildcards):
    import numpy
    import json
    #Use the checkpoint to check the stats file exists
    checkpoint_output = checkpoints.summary_table.get()
    vcf_stats = data_dir/"zarr_stats"/wildcards.vcf_name/"stats.json"
    with open(vcf_stats, "r") as json_stats_f:
        with open(wildcards.sample_ids_file.replace("--", "/"),'r') as f:
            n_samples = len(numpy.genfromtxt(f,dtype=str))
        stats = json.load(json_stats_f)
        n_ploidy = stats['n_ploidy']
        n_sites = stats['n_variants']
        n_masked = stats['sites_masked']
        ac1 = sum([ac == 1 for ac in stats['allele_counts']])
        print(f"Sites: {n_sites}, Masked: {n_masked}, AC1: {ac1}, Samples: {n_samples}, Ploidy: {n_ploidy}")
        mem = int(5_000 + (((n_sites-n_masked) - ac1) * n_samples * n_ploidy) / (8 * 1_048_576))
        print(f"Memory: {mem}MB")
        return mem

def get_ancestor_cpus(wildcards):
    import math
    #Use the checkpoint to check the stats file exists
    checkpoint_output = checkpoints.summary_table.get()
    #For the GEL cluster, at least, we must ask for more CPUs to get more memory
    return max(4, math.ceil(get_ancestor_gen_memory(wildcards) / 16_000))

rule generate_ancestors:
    input:
        data_dir/"zarr_vcfs_subsets"/"{vcf_name}-subset-{sample_ids_file}"/"data.zarr",
        data_dir/"zarr_stats"/"{vcf_name}"/"stats.json"
    output:
        data_dir/"ancestors"/"{vcf_name}-subset-{sample_ids_file}"/"ancestors.zarr"
    threads: get_ancestor_cpus
    resources:
        mem_mb=get_ancestor_gen_memory,
        time_min=24 * 60,
        runtime=24 * 60
    run:
        import tsinfer
        import logging
        logging.basicConfig(level=logging.INFO)
        sample_data = tsinfer.SgkitSampleData(input[0])
        #Limit the threads to 8 as it too many leads to contention
        os.makedirs(data_dir/"progress"/"generate_ancestors", exist_ok=True)
        with open(data_dir/"progress"/"generate_ancestors"/f"{wildcards.vcf_name}.log", "w") as log_f:
             tsinfer.generate_ancestors(
                 sample_data,
                 path=output[0],
                 genotype_encoding=1,
                 num_threads=max(threads,8),
                 progress_monitor=tsinfer.progress.ProgressMonitor(tqdm_kwargs={'file':log_f, 'mininterval':30})
             )

rule truncate_ancestors:
    input:
        data_dir/"ancestors"/"{vcf_name}-subset-{sample_ids_file}"/"ancestors.zarr"
    output:
        data_dir/"ancestors"/"{vcf_name}-subset-{sample_ids_file}"/"ancestors-truncated.zarr"
    threads: 1
    resources:
        mem_mb=16000,
        time_min=4 * 60,
        runtime=4 * 60
    run:
        import tsinfer
        import logging
        logging.basicConfig(level=logging.INFO)
        ancestors = tsinfer.AncestorData.load(input[0])
        truncated = ancestors.truncate_ancestors(0.4, 0.6, length_multiplier=1, path=output[0])

rule match_ancestors:
    input:
        data_dir/"ancestors"/"{vcf_name}-subset-{sample_ids_file}"/"ancestors-truncated.zarr",
        data_dir/"zarr_vcfs_subsets"/"{vcf_name}-subset-{sample_ids_file}"/"data.zarr",
    output:
        data_dir/"ancestors"/"{vcf_name}-subset-{sample_ids_file}"/"ancestors.trees",
        data_dir/"ancestors"/"{vcf_name}-subset-{sample_ids_file}"/"performance_report.html"
    #Minimal threads as we're using dask
    threads: 2
    resources:
        time_min=6 * 30 * 24 * 60, #6 months
        runtime=6 * 30 * 24 * 60
    run:
        import tsinfer
        import logging
        import msprime
        import numpy
        import os
        logging.basicConfig(level=logging.INFO)
        ancestors = tsinfer.AncestorData.load(input[0])
        sample_data = tsinfer.SgkitSampleData(input[1])
        os.makedirs(data_dir/"progress"/"match_ancestors", exist_ok=True)
        os.makedirs(data_dir / "resume" / "match_ancestors", exist_ok=True)
        client = Client(config["scheduler_address"])
        with open(data_dir/"progress"/"match_ancestors"/f"{wildcards.vcf_name}-{wildcards.sample_ids_file}.log", "w") as log_f:
            with performance_report(
                    filename=output[1]
            ):
                ts = tsinfer.match_ancestors(
                    sample_data,
                    ancestors,
                    path_compression=True,
                    num_threads=threads,
                    precision=15,
                    progress_monitor=tsinfer.progress.ProgressMonitor(tqdm_kwargs={'file':log_f, 'mininterval':30}),
                    resume_lmdb_file=str(data_dir/"resume"/"match_ancestors"/f"{wildcards.vcf_name}-{wildcards.sample_ids_file}.lmdb"),
                    dask=True,
                )
        ts.dump(output[0])

# rule match_samples:
#     input:
#         data_dir/"ancestors"/"{vcf_name}-subset-{sample_ids_file}"/"ancestors.zarr",
#         data_dir/"zarr_vcfs_subsets"/"{vcf_name}-subset-{sample_ids_file}"/"data.zarr",
#     output:
#         data_dir / "ancestors" / "{vcf_name}-subset-{sample_ids_file}" / "out.trees"
#
#     threads: 24
#     resources:
#         time_min=6 * 30 * 24 * 60, #6 months
#         runtime=6 * 30 * 24 * 60
#     run:
#         import tsinfer
#         import logging
#         logging.basicConfig(level=logging.INFO)
#
#         ancestors = tsinfer.AncestorData.load(input[0])
#         sample_data = tsinfer.SgkitSampleData(input[1])
#
#         inference_pos = ancestors.sites_position[:]
#
#         os.makedirs(data_dir/"progress"/"match_ancestors", exist_ok=True)
#         with open(data_dir/"progress"/"match_ancestors"/f"{wildcards.vcf_name}.log", "w") as log_f:
#             ts = tsinfer.match_ancestors(
#                 sample_data,
#                 ancestors,
#                 num_threads=threads,
#                 progress_monitor=tsinfer.progress.ProgressMonitor(tqdm_kwargs={'file':log_f, 'mininterval':30})
#             )
#         ts.dump(output[0])
