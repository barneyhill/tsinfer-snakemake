import sgkit
import sgkit.io.vcf
import os.path
import dask
from pathlib import Path
from dask.distributed import Client, performance_report

configfile: "config.yaml"
shell.prefix(config["prefix"])

for k,v in config["dask"].items():
    dask.config.set({k:v})

def number_to_SI(number):
    """Convert a number to a string with SI units"""
    units = ["", "K", "M", "G", "T", "P", "E", "Z", "Y"]
    unit = 0
    while number > 1000:
        number /= 1000
        unit += 1
    return f"{number:.2f}{units[unit]}"

data_dir=Path(config['data_dir'])

localrules: all, summary_table

rule all:
    input: data_dir/"vcf_summary_table.csv"

rule vcf_to_zarrs:
    input:
        vcf=lambda wildcards: config["vcfs"][wildcards.vcf_name]['vcf'],
        tbi=lambda wildcards: config["vcfs"][wildcards.vcf_name]['vcf']+".tbi"
    output:
        directory(data_dir/"zarr_vcfs"/"{vcf_name}"/"data.zarr"),
        data_dir/"zarr_vcfs"/"{vcf_name}"/"performance_report.html"
    resources:
        dask_cluster=1
    params:
        target_part_size="5M",
        read_chunk_length=10_000,
        temp_chunk_length=20_000,
        chunk_length=60_000,
        chunk_width=3_000,
        retain_temp_files=True
    run:
        client = Client(config["scheduler_address"])
        with performance_report(
            filename=output[1]
        ):
            sgkit.io.vcf.vcf_reader.vcf_to_zarr(
                input[0],
                output[0],
                target_part_size=params.target_part_size,
                read_chunk_length=params.read_chunk_length,
                temp_chunk_length=params.temp_chunk_length,
                chunk_length=params.chunk_length,
                chunk_width=params.chunk_width,
                tempdir=config["temp_dir"]
            )


rule zarr_stats:
    input:
        data_dir/"zarr_vcfs"/"{vcf_name}"/"data.zarr"
    output:
        data_dir/"zarr_stats"/"{vcf_name}"/"stats.json"
    resources:
        dask_cluster=1
    run:
        client = Client(config["scheduler_address"])
        ds = sgkit.load_dataset(input[0])
        out = {}
        out['dataset_summary'] = str(ds)
        out['name'] = ', '.join(ds.contigs)
        out['n_samples'] = ds.dims['samples']
        out['n_variants'] = ds.dims['variants']
        out['n_ploidy'] = ds.dims['ploidy']
        out['allele_counts'] = (ds.call_genotype == 1).sum(dim=['samples', 'ploidy']).to_numpy().tolist()
        out['missing_count'] = int((ds.call_genotype == -1).sum().to_numpy())
        out['tri_allele_and_above_count'] = int((ds.call_genotype > 1).sum().to_numpy())
        total_size = 0
        for dirpath, dirnames, filenames in os.walk(input[0]):
            for f in filenames:
                fp = os.path.join(dirpath, f)
                total_size += os.path.getsize(fp)
        out['size'] = total_size
        with open(output[0], "w") as f:
            f.write(json.dumps(out))

rule load_ancestral_fasta:
    input:
        lambda wildcards: config["vcfs"][wildcards.vcf_name]['ancestral_fasta'],
        data_dir/"zarr_vcfs"/"{vcf_name}"/"data.zarr"
    output:
        directory(data_dir/"zarr_vcfs"/"{vcf_name}"/"data.zarr"/"variant_ancestral_state")
    run:
        import pysam
        fasta = pysam.FastaFile(input[0])
        # NB! We put in an extra character at the start to convert to 1 based coords.
        codec = 'utf-32-le' if sys.byteorder == 'little' else 'utf-32-be'
        ancestral_sequence = "X" + fasta.fetch(reference=fasta.references[0])
        ancestral_sequence = np.frombuffer(bytearray(ancestral_sequence,codec), dtype="U1")
        # From the human ancestral states README:
        # The convention for the sequence is:
        #    ACTG : high-confidence call, ancestral state supported by other 2 sequences
        #    actg : low-confidence call, ancestral state supported by one sequence only
        #    N    : failure, the ancestral state is not supported by any other sequence
        #    -    : the extant species contains an insertion at this position
        #    .    : no coverage in the alignment
        ds = sg.load_dataset(input[1])
        ancestral_states = ancestral_sequence[ds['variant_position'].values]
        ancestral_states = xr.DataArray(data=ancestral_states, dims=["variants"], name="variant_ancestral_state")
        ds.update({"variant_ancestral_state": ancestral_states})
        sg.save_dataset(ds.drop_vars(set(ds.data_vars) - {"variant_ancestral_state"}), input[1], mode="a")

rule summary_table:
    input:
        expand(data_dir/"zarr_stats"/"{vcf_name}"/"stats.json", vcf_name=config["vcfs"].keys())
    output:
        data_dir/"vcf_summary_table.csv"
    run:
        with open(output[0], "w") as f:
           f.write("vcf_name, zarr_size, n_variants, n_samples, ac0, ac1, ac2, missing_count, tri_allele_and_above_count, full_nbytes, no_singletons_nbytes, half_no_singletons_nbytes\n")
           for vcf_stats in input:
              with open(vcf_stats, "r") as json_stats_f:
                stats = json.load(json_stats_f)
                ac1 = stats['allele_counts'][1]
                n_sites = stats['n_variants']
                n_samples = stats['n_samples']
                n_ploidy = stats['n_ploidy']
                f.write(f"{stats['name']}, ")
                f.write(', '.join([str(number_to_SI(n)) for n in [
                    stats['size'],
                    n_sites,
                    n_samples,
                    stats['allele_counts'][0],
                    ac1,
                    stats['allele_counts'][2],
                    stats['missing_count'],
                    stats['tri_allele_and_above_count'],
                    n_sites*n_samples*n_ploidy,
                    (n_sites-ac1)*n_samples*n_ploidy,
                    (n_sites-ac1)*n_samples*n_ploidy*0.5,
                ]]))
                f.write('\n')

