import os.path
import dask
from pathlib import Path
import steps

configfile: "config.yaml"
shell.prefix(config["prefix"])

for k,v in config["dask"].items():
    dask.config.set({k:v})

data_dir=Path(config['data_dir'])

# These are quick steps that we don't want to submit jobs for
localrules: all, summary_table

rule all:
    input:
        expand(data_dir/"{subset_name}-region_summary_table.csv", subset_name=config["sample_subsets"].keys()),
        expand(data_dir/ "trees" / "{subset_name}-{region_name}" / "{subset_name}-{region_name}-truncate-{truncation}-mm{mismatch}.trees",
            subset_name=config["sample_subsets"].keys(),
            region_name=config["regions"].keys(),
            mismatch=config["mismatch_values"],
            truncation=[f"{c['lower']}-{c['upper']}-{c['multiplier']}" for c in config['truncate']]
        )


rule vcf_to_zarrs:
    input:
        vcf=lambda wildcards: config["vcf"].format(chrom=wildcards.chrom_num),
        tbi=lambda wildcards: config["vcf"].format(chrom=wildcards.chrom_num)+".tbi"
    output:
        data_dir/"zarr_vcfs"/"chr{chrom_num}"/"data.zarr"/".vcf_done",
        data_dir/"zarr_vcfs"/"chr{chrom_num}"/"performance_report.html"
    resources:
        dask_cluster=10,
        mem_mb=16000,
        time_min=24 * 60,
        runtime=24 * 60
    params:
        target_part_size="5M",
        read_chunk_length=10_000,
        temp_chunk_length=20_000,
        chunk_length=40_000,
        chunk_width=1_500,
        retain_temp_files=True
    run:
        steps.vcf_to_zarrs(input, output, wildcards, config, params)

rule load_ancestral_fasta:
    input:
        lambda wildcards: config["ancestral_fasta"].format(chrom=wildcards.chrom_num),
        data_dir/"zarr_vcfs"/"chr{chrom_num}"/"data.zarr"/".vcf_done"
    output:
        directory(data_dir/"zarr_vcfs"/"chr{chrom_num}"/"data.zarr"/"variant_ancestral_allele")
    threads: 1
    resources:
        mem_mb=16000,
        time_min=4 * 60,
        runtime=4 * 60
    run:
        steps.load_ancestral_fasta(input, output, wildcards, config, params)

rule mask_sites:
    input:
        data_dir/"zarr_vcfs"/"chr{chrom_num}"/"data.zarr"/".vcf_done",
        data_dir/"zarr_vcfs"/"chr{chrom_num}"/"data.zarr"/"variant_ancestral_allele"
    output:
        directory(data_dir/"zarr_vcfs"/"chr{chrom_num}"/"data.zarr"/"variant_mask"),
        directory(data_dir/"zarr_vcfs"/"chr{chrom_num}"/"data.zarr"/"variant_duplicate_position_mask"),
        directory(data_dir/"zarr_vcfs"/"chr{chrom_num}"/"data.zarr"/"variant_bad_ancestral_mask"),
        directory(data_dir/"zarr_vcfs"/"chr{chrom_num}"/"data.zarr"/"variant_no_ancestral_allele_mask")
    resources:
        mem_mb=16000,
        time_min=4 * 60,
        runtime=4 * 60
    run:
        steps.mask_sites(input, output, wildcards, config, params)

rule subset_zarr_vcf:
    input:
        lambda wildcards: [(data_dir/"zarr_vcfs"/f"chr{steps.parse_region(config['regions'][wildcards.region_name])[0]}"/"data.zarr"/suffix)
           for suffix in [".vcf_done", "variant_ancestral_allele", "variant_mask",
                          "variant_duplicate_position_mask", "variant_bad_ancestral_mask",
                          "variant_no_ancestral_allele_mask"]
        ],
        lambda wildcards: config['sample_subsets'][wildcards.subset_name],
    output:
        directory(data_dir/"zarr_vcfs_subsets"/"{subset_name}-{region_name}"/"data.zarr")
    resources:
        dask_cluster=5,
        mem_mb=16000,
        time_min=4 * 60,
        runtime=4 * 60
    run:
        steps.subset_zarr_vcf(input, output, wildcards, config, params)


rule zarr_stats:
    input:
        data_dir/"zarr_vcfs_subsets"/"{subset_name}-{region_name}"/"data.zarr"
    output:
        data_dir/"zarr_stats"/"{subset_name}-{region_name}"/"stats.json"
    resources:
        dask_cluster=5,
        mem_mb=16000,
        time_min=4 * 60,
        runtime=4 * 60
    run:
        steps.zarr_stats(input, output, wildcards, config, params)


checkpoint summary_table:
    input:
        lambda wildcards: [data_dir/"zarr_stats"/f"{wildcards.subset_name}-{region_name}"/"stats.json" for region_name in config["regions"].keys()]
    output:
        data_dir/"{subset_name}-region_summary_table.csv"
    run:
        steps.summary_table(input, output, wildcards, config, params)

def get_ancestor_gen_memory(wildcards):
    import numpy
    import json
    #Use the checkpoint to check the stats file exists
    checkpoint_output = checkpoints.summary_table.get(subset_name=wildcards.subset_name)
    region_stats = data_dir / "zarr_stats" / f"{wildcards.subset_name}-{wildcards.region_name}" / "stats.json"
    with open(region_stats, "r") as json_stats_f:
        with open(config['sample_subsets'][wildcards.subset_name],'r') as f:
            n_samples = len(numpy.genfromtxt(f,dtype=str))
        stats = json.load(json_stats_f)
        n_ploidy = stats['n_ploidy']
        n_sites = stats['n_variants']
        n_masked = stats['sites_masked']
        ac1 = sum([ac == 1 for ac in stats['allele_counts']])
        mem = int(16_000 + (((n_sites-n_masked) - ac1) * n_samples * n_ploidy) / (8 * 1_048_576))
        return mem


rule generate_ancestors:
    input:
        data_dir/"zarr_vcfs_subsets"/"{subset_name}-{region_name}"/"data.zarr",
        data_dir/"zarr_stats"/"{subset_name}-{region_name}"/"stats.json"
    output:
        data_dir/"ancestors"/"{subset_name}-{region_name}"/"ancestors.zarr"
    threads: config['max_threads']
    resources:
        mem_mb=get_ancestor_gen_memory,
        time_min=24 * 60,
        runtime=24 * 60
    run:
        import tsinfer
        import logging
        logging.basicConfig(level=logging.INFO)
        sample_data = tsinfer.SgkitSampleData(input[0])
        os.makedirs(data_dir/"progress"/"generate_ancestors", exist_ok=True)
        with open(data_dir/"progress"/"generate_ancestors"/f"{wildcards.subset_name}-{wildcards.region_name}.log", "w") as log_f:
             tsinfer.generate_ancestors(
                 sample_data,
                 path=output[0],
                 genotype_encoding=1,
                 num_threads=threads,
                 progress_monitor=tsinfer.progress.ProgressMonitor(tqdm_kwargs={'file':log_f, 'mininterval':30})
             )

rule truncate_ancestors:
    input:
        data_dir/"ancestors"/"{subset_name}-{region_name}"/"ancestors.zarr"
    output:
        data_dir/"ancestors"/"{subset_name}-{region_name}"/"ancestors-truncate-{lower}-{upper}-{multiplier}.zarr"
    threads: 1
    resources:
        mem_mb=16000,
        time_min=4 * 60,
        runtime=4 * 60
    run:
        import tsinfer
        import logging
        import shutil
        lower = float(wildcards.lower)
        upper = float(wildcards.upper)
        multiplier = float(wildcards.multiplier)

        logging.basicConfig(level=logging.INFO)
        if lower == 0 and upper == 0 and multiplier == 0:
            #No truncation, just copy the file using the OS
            shutil.copy(input[0], output[0])
        else:
            ancestors = tsinfer.AncestorData.load(input[0])
            truncated = ancestors.truncate_ancestors(lower, upper, length_multiplier=multiplier, path=output[0])

rule match_ancestors:
    input:
        data_dir/"ancestors"/"{subset_name}-{region_name}"/"ancestors-truncate-{lower}-{upper}-{multiplier}.zarr",
        data_dir/"zarr_vcfs_subsets"/"{subset_name}-{region_name}"/"data.zarr",
    output:
        data_dir/"ancestors"/"{subset_name}-{region_name}"/"ancestors-truncate-{lower}-{upper}-{multiplier}.trees",
        data_dir/"ancestors"/"{subset_name}-{region_name}"/"ancestors-truncate-{lower}-{upper}-{multiplier}-performance_report.html"
    threads: config['max_threads']
    resources:
        mem_mb = config['max_mem'],
        time_min=6 * 30 * 24 * 60, #6 months
        runtime=6 * 30 * 24 * 60
    run:
        steps.match_ancestors(input, output, wildcards, config, threads)


rule match_samples:
    input:
        data_dir/"ancestors"/"{subset_name}-{region_name}"/"ancestors-truncate-{lower}-{upper}-{multiplier}.trees",
        data_dir/"zarr_vcfs_subsets"/"{subset_name}-{region_name}"/"data.zarr",
        lambda wildcards: config['recomb_map'].format(chrom=steps.parse_region(config['regions'][wildcards.region_name])[0])
    output:
        data_dir/"trees"/"{subset_name}-{region_name}"/"{subset_name}-{region_name}-truncate-{lower}-{upper}-{multiplier}-mm{mismatch}.trees",
        data_dir/"samples"/"{subset_name}-{region_name}-truncate-{lower}-{upper}-{multiplier}-mm{mismatch}"/"performance_report.html"
    #Minimal threads as we're using dask
    threads: 2
    resources:
        mem_mb = 32000,
        time_min=6 * 30 * 24 * 60, #6 months
        runtime=6 * 30 * 24 * 60
    run:
        steps.match_samples(input, output, wildcards, config, threads)