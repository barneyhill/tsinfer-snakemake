import sgkit
import sgkit.io.vcf
import os.path
from pathlib import Path
from dask.distributed import Client, performance_report

configfile: "config.yaml"
shell.prefix(config["prefix"])

def make_dask_cluster():
    # Make a dask cluster from the module.class string specified in config
    cluster_class = config["cluster"]["class"]
    cluster_module, cluster_class = cluster_class.rsplit(".", 1)
    cluster_module = __import__(cluster_module, fromlist=[cluster_class])
    cluster_class = getattr(cluster_module, cluster_class)
    cluster = cluster_class(**config["cluster"].get("kwargs", {}))
    if "adapt" in config["cluster"]:
        cluster.adapt(**config["cluster"]["adapt"])
    print(f"Created cluster {cluster.__class__.__name__} at {cluster.dashboard_link}")
    return cluster.scheduler_address
scheduler_address = make_dask_cluster()

def vcf_name(vcf_file):
    return os.path.basename(vcf_file).split(".")[0]

def number_to_SI(number):
    """Convert a number to a string with SI units"""
    units = ["", "K", "M", "G", "T", "P", "E", "Z", "Y"]
    unit = 0
    while number > 1000:
        number /= 1000
        unit += 1
    return f"{number:.2f}{units[unit]}"

vcfs={vcf_name(vcf_file):vcf_file for vcf_file in config["vcf_files"]}
data_dir=Path(config['data_dir'])

# These are quick steps that we don't want to run on the cluster
localrules: all, vcf_to_zarrs, zarr_stats, summary_table

rule all:
    input: data_dir/"vcf_summary_table.csv"

rule vcf_to_zarrs:
    input:
        vcf=lambda wildcards: vcfs[wildcards.vcf_name],
        tbi=lambda wildcards: vcfs[wildcards.vcf_name]+".tbi"
    output:
        directory(data_dir/"zarr_vcfs"/"{vcf_name}"/"data.zarr"),
        data_dir/"zarr_vcfs"/"{vcf_name}"/"performance_report.html"
    resources:
        dask_cluster=1
    params:
        target_part_size="5M",
        read_chunk_length=10_000,
        temp_chunk_length=25_000,
        chunk_length=50_000,
        chunk_width=3_000
    run:
        client = Client(scheduler_address)
        with performance_report(
            filename=output[1]
        ):
            sgkit.io.vcf.vcf_reader.vcf_to_zarr(
                input[0],
                output[0],
                target_part_size=params.target_part_size,
                read_chunk_length=params.read_chunk_length,
                temp_chunk_length=params.temp_chunk_length,
                chunk_length=params.chunk_length,
                chunk_width=params.chunk_width,
                tempdir=config["temp_dir"]
            )


rule zarr_stats:
    input:
        data_dir/"zarr_vcfs"/"{vcf_name}"/"data.zarr"
    output:
        data_dir/"zarr_stats"/"{vcf_name}"/"stats.json"
    resources:
        dask_cluster=1
    run:
        client = Client(scheduler_address)
        ds = sgkit.load_dataset(input[0])
        out = {}
        out['dataset_summary'] = str(ds)
        out['name'] = ', '.join(ds.contigs)
        out['n_samples'] = ds.dims['samples']
        out['n_variants'] = ds.dims['variants']
        out['n_ploidy'] = ds.dims['ploidy']
        out['allele_counts'] = (ds.call_genotype == 1).sum(dim=['samples', 'ploidy']).to_numpy().tolist()
        with open(output[0], "w") as f:
            f.write(json.dumps(out))

rule summary_table:
    input:
        expand(data_dir/"zarr_stats"/"{vcf_name}"/"stats.json", vcf_name=vcfs.keys())
    output:
        data_dir/"vcf_summary_table.csv"
    run:
        with open(output[0], "w") as f:
           f.write("vcf_name, n_variants, n_samples, ac0, ac1, ac2, full_nbytes, no_singletons_nbytes, half_no_singletons_nbytes\n")
           for vcf_stats in input:
              with open(vcf_stats, "r") as json_stats_f:
                stats = json.load(json_stats_f)
                ac1 = stats['allele_counts'][1]
                n_sites = stats['n_variants']
                n_samples = stats['n_samples']
                n_ploidy = stats['n_ploidy']
                f.write(f"{stats['name']}, ")
                f.write(', '.join([str(number_to_SI(n)) for n in [
                    n_sites,
                    n_samples,
                    stats['allele_counts'][0],
                    ac1,
                    stats['allele_counts'][2],
                    n_sites*n_samples*n_ploidy,
                    (n_sites-ac1)*n_samples*n_ploidy,
                    (n_sites-ac1)*n_samples*n_ploidy*0.5,
                ]]))
                f.write('\n')

